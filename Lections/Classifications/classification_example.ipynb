{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import Normalizer, OneHotEncoder, StandardScaler\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from skopt import BayesSearchCV\n",
    "from scipy.stats import zscore\n",
    "from tqdm import tqdm\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "\n",
    "def categorial_encoding(X_train, X_test, encoders=None):\n",
    "    \"\"\"\n",
    "    Encodes categorical features using One-Hot Encoding.\n",
    "    Handles cases where categories might be missing in one dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train (pd.DataFrame): Training features.\n",
    "    - X_test (pd.DataFrame): Testing features.\n",
    "    - encoders (dict): Dictionary of already-fitted encoders (optional).\n",
    "\n",
    "    Returns:\n",
    "    - Tuple[pd.DataFrame, pd.DataFrame, dict]: Transformed X_train, X_test, and dictionary of encoders.\n",
    "    \"\"\"\n",
    "    categorical_columns = X_train.select_dtypes(include=['object']).columns\n",
    "    if encoders is None:\n",
    "        encoders = {}\n",
    "\n",
    "    for col in categorical_columns:\n",
    "        num_categories = X_train[col].nunique()\n",
    "        # TODO if num_categories is large OneHotEncoding works bad and there are alternatives\n",
    "        # if num_categories > 10:\n",
    "        # ...\n",
    "\n",
    "        encoder = OneHotEncoder(drop='first', sparse_output=False, handle_unknown=\"infrequent_if_exist\")\n",
    "        encoder.fit(X_train[[col]])\n",
    "\n",
    "        # Get expected columns from encoder\n",
    "        expected_cols = encoder.get_feature_names_out([col])\n",
    "        \n",
    "        # Transform both datasets\n",
    "        train_encoded = encoder.transform(X_train[[col]])\n",
    "        test_encoded = encoder.transform(X_test[[col]])\n",
    "        \n",
    "        # Create DataFrames with consistent columns\n",
    "        train_encoded_df = pd.DataFrame(train_encoded, columns=expected_cols, index=X_train.index)\n",
    "        test_encoded_df = pd.DataFrame(test_encoded, columns=expected_cols, index=X_test.index)\n",
    "        \n",
    "        # Drop original column and join encoded data\n",
    "        X_train = X_train.drop(columns=col).join(train_encoded_df)\n",
    "        X_test = X_test.drop(columns=col).join(test_encoded_df)\n",
    "        \n",
    "        encoders[col] = ('onehot', encoder, expected_cols.tolist())\n",
    "\n",
    "    return X_train, X_test, encoders\n",
    "\n",
    "\n",
    "def handle_missing_data(\n",
    "    train_df: pd.DataFrame, \n",
    "    test_df: Optional[pd.DataFrame] = None\n",
    ") -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Fills missing values in the training and (optionally) test DataFrame without dropping any rows.\n",
    "\n",
    "    - For numeric columns (int or float), missing values are filled with the column mean.\n",
    "    - For categorical columns (object or other types), missing values are filled with the column mode.\n",
    "    - The same fill values from the training set are applied to the test set (if provided), ensuring consistency.\n",
    "\n",
    "    Parameters:\n",
    "        train_df (pd.DataFrame): The training dataset with possible missing values.\n",
    "        test_df (Optional[pd.DataFrame]): The test dataset with possible missing values. Default is None.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, Optional[pd.DataFrame]]: The training and (optionally) test DataFrames with missing values handled.\n",
    "    \"\"\"\n",
    "    # Copy to avoid modifying original data\n",
    "    train_df = train_df.copy()\n",
    "    if test_df is not None:\n",
    "        test_df = test_df.copy()\n",
    "\n",
    "    # Fill missing values in training data\n",
    "    for col in train_df.columns:\n",
    "        if train_df[col].dtype in ['float64', 'int64']:\n",
    "            fill_value = train_df[col].median()\n",
    "        else:\n",
    "            fill_value = train_df[col].mode()\n",
    "        train_df[col] = train_df[col].fillna(fill_value)\n",
    "        \n",
    "        # Fill missing values in test data using same fill values\n",
    "        if test_df is not None and col in test_df:\n",
    "            test_df[col] = test_df[col].fillna(fill_value)\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def drop_outliers(X_train, y_train, threshold=5):\n",
    "    \"\"\"\n",
    "    Filters out rows from training data with Z-score above threshold (outliers).\n",
    "\n",
    "    Parameters:\n",
    "    - X_train (pd.DataFrame): Training features.\n",
    "    - y_train (pd.Series): Training diabetess.\n",
    "    - threshold (float): Z-score threshold for detecting outliers.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple[pd.DataFrame, pd.Series]: Filtered training data.\n",
    "    \"\"\"\n",
    "    numeric_train = X_train.select_dtypes(include=[np.number])\n",
    "    z_train = np.abs(zscore(numeric_train))\n",
    "    mask = (z_train < threshold).all(axis=1)\n",
    "\n",
    "    return X_train[mask], y_train[mask]\n",
    "\n",
    "\n",
    "def scale_features(X_train, X_test, scaler=None):\n",
    "    \"\"\"\n",
    "    Applies feature scaling using StandardScaler.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train (pd.DataFrame): Training features.\n",
    "    - X_test (pd.DataFrame): Test features.\n",
    "    - scaler (StandardScaler): Optionally provide pre-fitted scaler.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple[pd.DataFrame, pd.DataFrame, StandardScaler]: Scaled train and test data, and fitted scaler.\n",
    "    \"\"\"\n",
    "    scaler = scaler or StandardScaler()\n",
    "    X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "    X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "    return X_train_scaled, X_test_scaled, scaler\n",
    "\n",
    "def normalize_features(X_train, X_test, normalizer=None):\n",
    "    \"\"\"\n",
    "    Applies L2 normalization to the feature vectors (rows).\n",
    "\n",
    "    Parameters:\n",
    "    - X_train (pd.DataFrame): Training features.\n",
    "    - X_test (pd.DataFrame): Test features.\n",
    "    - normalizer (Normalizer): Optionally provide pre-fitted Normalizer.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple[pd.DataFrame, pd.DataFrame, Normalizer]: Normalized train and test data, and fitted normalizer.\n",
    "    \"\"\"\n",
    "    normalizer = normalizer or Normalizer(norm='l2')\n",
    "    X_train_norm = pd.DataFrame(normalizer.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "    X_test_norm = pd.DataFrame(normalizer.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
    "    return X_train_norm, X_test_norm, normalizer\n",
    "\n",
    "\n",
    "f1 = make_scorer(f1_score , average='macro')\n",
    "\n",
    "def train_model(X_train, y_train, model, param_space, n_iterations=10, n_splits=5, scoring='accuracy'):\n",
    "    \"\"\"\n",
    "    Trains a model using Bayesian hyperparameter optimization with k-fold cross validation.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train (pd.DataFrame/array): Training features\n",
    "    - y_train (pd.Series/array): Training targets\n",
    "    - model: Scikit-learn compatible model/estimator\n",
    "    - param_space (dict): Parameter search space for Bayesian optimization\n",
    "    - n_iterations (int): Number of optimization iterations\n",
    "    - n_splits (int): Number of folds for cross validation\n",
    "    - scoring (str/callable): Scoring metric to optimize for\n",
    "    \n",
    "    Returns:\n",
    "    - BayesSearchCV: Fitted optimizer with best parameters and cross-validation results\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set up stratified k-fold (preserves class distribution in splits)\n",
    "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "            \n",
    "    # Bayesian optimization with cross-validation\n",
    "    opt = BayesSearchCV(\n",
    "        estimator=model,\n",
    "        search_spaces=param_space,\n",
    "        n_iter=n_iterations,\n",
    "        cv=cv,\n",
    "        scoring=scoring,\n",
    "        n_jobs=-1,\n",
    "        verbose=0,\n",
    "        random_state=42,\n",
    "        return_train_score=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Starting Bayesian Optimization with {n_splits}-fold CV...\")\n",
    "    with tqdm(total=n_iterations, desc=\"Bayesian Optimization Progress\") as pbar:\n",
    "        def on_step(optim_result):\n",
    "            pbar.update(1)\n",
    "        opt.fit(X_train, y_train, callback=on_step)\n",
    "    \n",
    "    # Print comprehensive results\n",
    "    print(\"\\nOptimization complete!\")\n",
    "    print(f\"Best parameters: {opt.best_params_}\")\n",
    "    print(f\"Best {scoring} score: {opt.best_score_:.4f}\")\n",
    "    print(f\"Best estimator: {opt.best_estimator_}\")    \n",
    "    print(\"---------------------------------------\")\n",
    "    \n",
    "    # Analyze cross-validation performance\n",
    "    cv_results = opt.cv_results_\n",
    "    best_index = opt.best_index_\n",
    "    best_train_score = cv_results['mean_train_score'][best_index]\n",
    "    best_validation_score = opt.best_score_\n",
    "\n",
    "    print(f\"\\nCross-validation summary ({n_splits}-fold):\")\n",
    "    print(f\"  - Best training score:    {best_train_score:.4f}\")\n",
    "    print(f\"  - Best validation score:  {best_validation_score:.4f}\")\n",
    "    # print(f\"Standard deviation: {np.std(cv_results['mean_test_score']):.4f}\")\n",
    "    return opt\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    \"\"\"\n",
    "    Prints the macro F1 score for the given model and dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained model.\n",
    "    - X (pd.DataFrame): Features.\n",
    "    - y (pd.Series): True Surviveds.\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X)\n",
    "    print(\"Macro F1 Score:\", f1_score(y, y_pred, average='macro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Load data\n",
    "data = pd.read_csv(r\"C:\\Users\\Timo\\OneDrive\\REDI School\\s25\\Session 14 - ML Classification - Lab Class\\data\\diabetes_prediction_dataset.csv\", sep=',')\n",
    "X = data.copy()\n",
    "y = data['diabetes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Preprocessing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_test = handle_missing_data(X_train, X_test)\n",
    "X_train, y_train = drop_outliers(X_train, y_train)    # TODO: Test if replacing with mmean/mode is better\n",
    "X_train, X_test, encoders = categorial_encoding(X_train, X_test)\n",
    "\n",
    "X_train = X_train.drop(columns=['diabetes'], errors='ignore')\n",
    "X_test = X_test.drop(columns=['diabetes'], errors='ignore')\n",
    "\n",
    "X_train, X_test, scaler = scale_features(X_train, X_test)\n",
    "#X_train, X_test, normalizer = normalize_features(X_train, X_test) # not all ML models need normalization. Test if it improves results. Should help with SVM and not with tree-based models.\n",
    "\n",
    "# TODO: remove multicolonarity columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Bayesian Optimization with 5-fold CV...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bayesian Optimization Progress: 100%|██████████| 10/10 [00:05<00:00,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimization complete!\n",
      "Best parameters: OrderedDict({'C': 8.37550967197731})\n",
      "Best make_scorer(f1_score, response_method='predict', average=macro) score: 0.7250\n",
      "Best estimator: LogisticRegression(C=8.37550967197731, class_weight='balanced', max_iter=1000)\n",
      "---------------------------------------\n",
      "\n",
      "Cross-validation summary (5-fold):\n",
      "  - Best training score:    0.7247\n",
      "  - Best validation score:  0.7250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Model training and evaluation LogisticRegression\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(class_weight='balanced',  max_iter=1000)\n",
    "param_space = {\n",
    "    'C': (0.01, 10)  # Regularization strength\n",
    "}\n",
    "model = train_model(X_train, y_train, model, param_space, n_iterations=10, scoring=f1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Bayesian Optimization with 5-fold CV...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bayesian Optimization Progress: 100%|██████████| 10/10 [00:28<00:00,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimization complete!\n",
      "Best parameters: OrderedDict({'C': 8.12583592369006, 'gamma': 0.17269968983516415})\n",
      "Best make_scorer(f1_score, response_method='predict', average=macro) score: 0.8453\n",
      "Best estimator: SVC(C=8.12583592369006, gamma=0.17269968983516415, probability=True)\n",
      "---------------------------------------\n",
      "\n",
      "Cross-validation summary (5-fold):\n",
      "  - Best training score:    0.8519\n",
      "  - Best validation score:  0.8453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Atttention: runs very long on whole dataset!!!\n",
    "# needs data normalisation\n",
    "\n",
    "# 3. Model training and evaluation Support Vector Machine\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC( probability=True)\n",
    "param_space = {\n",
    "    'C': (0.01, 10),\n",
    "    'gamma': (0.001, 1)\n",
    "}\n",
    "# only some data as SVM are slow with a lot of data\n",
    "model = train_model(X_train[:10000], y_train[:10000], model, param_space, n_iterations=10, scoring=f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Bayesian Optimization with 5-fold CV...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bayesian Optimization Progress: 100%|██████████| 10/10 [00:09<00:00,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimization complete!\n",
      "Best parameters: OrderedDict({'max_depth': 1, 'n_estimators': 41})\n",
      "Best make_scorer(f1_score, response_method='predict', average=macro) score: 0.7932\n",
      "Best estimator: RandomForestClassifier(class_weight='balanced', max_depth=1, n_estimators=41)\n",
      "---------------------------------------\n",
      "\n",
      "Cross-validation summary (5-fold):\n",
      "  - Best training score:    0.7934\n",
      "  - Best validation score:  0.7932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Model training and evaluation RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(class_weight='balanced')\n",
    "param_space = {    \n",
    "    'max_depth': (1, 10),\n",
    "    'n_estimators': (2, 50)\n",
    "}\n",
    "model = train_model(X_train, y_train, model, param_space, n_iterations=10, scoring=f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Bayesian Optimization with 5-fold CV...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bayesian Optimization Progress: 100%|██████████| 10/10 [00:05<00:00,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimization complete!\n",
      "Best parameters: OrderedDict({'max_depth': 6, 'n_estimators': 46})\n",
      "Best make_scorer(f1_score, response_method='predict', average=macro) score: 0.8911\n",
      "Best estimator: XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              feature_weights=None, gamma=None, grow_policy=None,\n",
      "              importance_type=None, interaction_constraints=None,\n",
      "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
      "              max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
      "              max_leaves=None, min_child_weight=None, missing=nan,\n",
      "              monotone_constraints=None, multi_strategy=None, n_estimators=46,\n",
      "              n_jobs=None, num_parallel_tree=None, ...)\n",
      "---------------------------------------\n",
      "\n",
      "Cross-validation summary (5-fold):\n",
      "  - Best training score:    0.8990\n",
      "  - Best validation score:  0.8911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Model training and evaluation XGBoostClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "model = XGBClassifier()\n",
    "param_space = {\n",
    "    'max_depth': (1, 10),\n",
    "    'n_estimators': (2, 50)\n",
    "}\n",
    "model = train_model(X_train, y_train, model, param_space, n_iterations=10, scoring=f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogisticRegression: validation score:      0.7520\n",
    "Support Vector Machine: validation score:  0.8453\n",
    "RandomForestClassifier: validation score:  0.7932\n",
    "XGB: validation score:                     0.8936"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
