{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_a_dataset():\n",
    "    np.random.seed(42)\n",
    "    n_samples = 200\n",
    "\n",
    "    # Independent base features\n",
    "    A = np.random.normal(0, 1, n_samples)\n",
    "    B = np.random.normal(0, 1, n_samples)\n",
    "    C = np.random.normal(0, 1, n_samples)\n",
    "\n",
    "    # Construct features that are combinations but with less obvious pairwise correlation\n",
    "    X1 = A + np.random.normal(0, 1, n_samples)\n",
    "    X2 = B + np.random.normal(0, 1, n_samples)\n",
    "    X3 = C + np.random.normal(0,  1, n_samples)\n",
    "\n",
    "    # Create complex collinearity: X4 ‚âà X1 + X2, X5 ‚âà X2 + X3, X6 ‚âà X1 + X3\n",
    "    X4 = X1 + X2 + np.random.normal(0, 1, n_samples)\n",
    "    X5 = X2 + X3 + np.random.normal(0,  1, n_samples)\n",
    "    X6 = X1 + X3 + np.random.normal(0,  1, n_samples)\n",
    "\n",
    "    # Target depends on X1 and X3\n",
    "    y = X1 +  X3 + np.random.normal(0, 1, n_samples)\n",
    "\n",
    "    # Create DataFrame\n",
    "    dataset = pd.DataFrame({\n",
    "        'X1': X1, 'X2': X2, 'X3': X3,\n",
    "        'X4': X4, 'X5': X5, 'X6': X6,\n",
    "        'y': y\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "    # Features (alle Spalten au√üer der letzten)\n",
    "    X = dataset.iloc[:, :-1]\n",
    "\n",
    "    # Target (die letzte Spalte)\n",
    "    y = dataset.iloc[:, -1]\n",
    "    return X,y\n",
    "X,y = create_a_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform X\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features to be dropped due to high correlation with other feature: []\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# preprcess data\n",
    "def multicollinearity_eliminator(numerical_data,threshold = 0.7):\n",
    "        \n",
    "        '''\n",
    "        Remove multicollinearity among the dataset features\n",
    "        \n",
    "        Parameters: \n",
    "        - numerical_data : Numeric features of the dataset\n",
    "        - threshold : the value indicates that if the features are X% correlated, drop them. Can be altered based on the requirements\n",
    "        '''\n",
    "        corr_matrix = numerical_data.corr().abs()\n",
    "        sns.heatmap(corr_matrix,cmap=sns.diverging_palette(230, 20, as_cmap=True),square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape),k=1).astype(np.bool_))\n",
    "        features_to_drop = [col for col in upper.columns if any(upper[col] >= threshold)]\n",
    "        print(\"Features to be dropped due to high correlation with other feature:\",features_to_drop)\n",
    "        numerical_data = numerical_data.drop(columns=features_to_drop)\n",
    "\n",
    "        return numerical_data\n",
    "\n",
    "X = multicollinearity_eliminator(X.select_dtypes(include=['int64', 'float64']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression - Training MSE: 0.8563\n",
      "Linear Regression - Test MSE:     1.306368\n"
     ]
    }
   ],
   "source": [
    "# Train a simple normal Linear Regression Model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lr = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_train_pred = lr.predict(X_train)\n",
    "y_test_pred = lr.predict(X_test)\n",
    "\n",
    "print(f\"Linear Regression - Training MSE: {mean_squared_error(y_train, y_train_pred):.4f}\")\n",
    "print(f\"Linear Regression - Test MSE:     {mean_squared_error(y_test, y_test_pred):4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have overfitting as MSE is much higher on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression - Training MSE: 0.8579\n",
      "Ridge Regression - Test MSE:     1.2753\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge = Ridge(alpha=1).fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = ridge.predict(X_train)\n",
    "y_test_pred = ridge.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Ridge Regression - Training MSE: {mean_squared_error(y_train, y_train_pred):.4f}\")\n",
    "print(f\"Ridge Regression - Test MSE:     {mean_squared_error(y_test, y_test_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ridge regression with alpha/lambda reduces overfitting and improves quality on test.\n",
    "\n",
    "But which alpha is the best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† **What the next Code Does**\n",
    "\n",
    "In this code, we apply **Ridge Regression** (a regularized form of linear regression) to a small dataset in order to prevent overfitting. We:\n",
    "\n",
    "1. Use **5-fold cross-validation** to evaluate model performance for different values of the regularization parameter `alpha`.\n",
    "2. For each alpha, we calculate the **Mean Squared Error (MSE)** on the validation fold and store the results.\n",
    "3. After testing all alpha values, we select the one with the **lowest average MSE** across folds.\n",
    "4. Finally, we train a Ridge model on the **entire training set using the best alpha from cross validation**, make predictions, and evaluate its performance on both training and test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation results for each alpha and fold:\n",
      "    alpha  fold       mse\n",
      "0    0.01     1  1.282890\n",
      "1    0.01     2  1.087608\n",
      "2    0.01     3  0.883461\n",
      "3    0.01     4  0.657579\n",
      "4    0.01     5  0.877125\n",
      "..    ...   ...       ...\n",
      "60  50.00     1  1.147454\n",
      "61  50.00     2  1.419053\n",
      "62  50.00     3  1.534254\n",
      "63  50.00     4  0.774231\n",
      "64  50.00     5  1.030508\n",
      "\n",
      "[65 rows x 3 columns]\n",
      "\n",
      "‚úÖ Best alpha based on average MSE: 1\n",
      "\n",
      "üìä Final Model Performance:\n",
      "Training MSE: 0.8579\n",
      "Test MSE:     1.2753\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Convert DataFrames to NumPy arrays\n",
    "X_array = X_train.values\n",
    "y_array = y_train.values\n",
    "\n",
    "# Define the list of regularization strengths to try\n",
    "alpha_values = [0.01,0.05,0.1, 0.2, 0.5, 0.75, 1, 1.5, 2, 5, 10, 20,50]\n",
    "\n",
    "# Set up 5-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Store all MSE results and average MSE per alpha\n",
    "cv_results = []\n",
    "average_mse_per_alpha = {}\n",
    "\n",
    "# Loop through each alpha value\n",
    "for alpha in alpha_values:\n",
    "    mse_per_fold = []\n",
    "\n",
    "    # Perform cross-validation\n",
    "    for fold_index, (train_idx, val_idx) in enumerate(kf.split(X_array), start=1):\n",
    "        # Split the data\n",
    "        X_train_fold, X_val_fold = X_array[train_idx], X_array[val_idx]\n",
    "        y_train_fold, y_val_fold = y_array[train_idx], y_array[val_idx]\n",
    "\n",
    "        # Train Ridge Regression model\n",
    "        model = Ridge(alpha=alpha, random_state=42)\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "        # Predict and calculate MSE\n",
    "        y_pred_fold = model.predict(X_val_fold)\n",
    "        mse = mean_squared_error(y_val_fold, y_pred_fold)\n",
    "\n",
    "        # Save results\n",
    "        mse_per_fold.append(mse)\n",
    "        cv_results.append({\n",
    "            'alpha': alpha,\n",
    "            'fold': fold_index,\n",
    "            'mse': mse\n",
    "        })\n",
    "\n",
    "    # Compute average MSE for current alpha\n",
    "    average_mse_per_alpha[alpha] = np.mean(mse_per_fold)\n",
    "\n",
    "# Print MSE results for all folds\n",
    "print(\"Cross-validation results for each alpha and fold:\")\n",
    "print(pd.DataFrame(cv_results))\n",
    "\n",
    "# Determine the best alpha (smallest average MSE)\n",
    "best_alpha = min(average_mse_per_alpha, key=average_mse_per_alpha.get)\n",
    "print(f\"\\n‚úÖ Best alpha based on average MSE: {best_alpha}\")\n",
    "\n",
    "# Train final model on full training data with best alpha\n",
    "final_model = Ridge(alpha=best_alpha, random_state=42)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on training and test sets\n",
    "y_train_pred = final_model.predict(X_train)\n",
    "y_test_pred = final_model.predict(X_test)\n",
    "\n",
    "# Show training and test MSE\n",
    "print(f\"\\nüìä Final Model Performance:\")\n",
    "print(f\"Training MSE: {mean_squared_error(y_train, y_train_pred):.4f}\")\n",
    "print(f\"Test MSE:     {mean_squared_error(y_test, y_test_pred):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìå **Observation**: The model's performance changes depending on the train-test split, which is common for small datasets.\n",
    "\n",
    "üèÜ **Best Alpha**: Based on the lowest average MSE across folds, the optimal alpha value was **1**.\n",
    "\n",
    "üìà **Final Model Performance**:\n",
    "- **Training MSE**: 0.8579\n",
    "- **Test MSE**: 1.2753\n",
    "\n",
    "This indicates that the model fits the training data reasonably well, and generalizes acceptably to the test set, considering the small sample size. And the MSE is lower with Ridge than with simple linear regression, which means regularisation helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation results for each alpha and fold:\n",
      "    alpha  fold       mse\n",
      "0    0.01     1  1.174334\n",
      "1    0.01     2  1.075146\n",
      "2    0.01     3  0.913354\n",
      "3    0.01     4  0.641930\n",
      "4    0.01     5  0.881524\n",
      "5    0.05     1  1.080602\n",
      "6    0.05     2  1.082993\n",
      "7    0.05     3  0.992542\n",
      "8    0.05     4  0.574165\n",
      "9    0.05     5  0.885542\n",
      "10   0.10     1  1.095086\n",
      "11   0.10     2  1.130185\n",
      "12   0.10     3  1.060289\n",
      "13   0.10     4  0.567448\n",
      "14   0.10     5  0.886798\n",
      "15   0.20     1  1.141200\n",
      "16   0.20     2  1.255398\n",
      "17   0.20     3  1.237486\n",
      "18   0.20     4  0.582461\n",
      "19   0.20     5  0.920577\n",
      "20   0.50     1  1.664080\n",
      "21   0.50     2  1.909527\n",
      "22   0.50     3  2.102059\n",
      "23   0.50     4  0.889593\n",
      "24   0.50     5  1.375227\n",
      "25   0.75     1  2.511141\n",
      "26   0.75     2  2.697822\n",
      "27   0.75     3  2.718350\n",
      "28   0.75     4  1.329517\n",
      "29   0.75     5  2.066655\n",
      "30   1.00     1  3.130301\n",
      "31   1.00     2  3.241495\n",
      "32   1.00     3  3.092617\n",
      "33   1.00     4  1.606618\n",
      "34   1.00     5  2.508409\n",
      "35   1.50     1  4.745116\n",
      "36   1.50     2  4.708570\n",
      "37   1.50     3  4.231595\n",
      "38   1.50     4  2.629709\n",
      "39   1.50     5  3.692695\n",
      "40   2.00     1  5.371057\n",
      "41   2.00     2  5.400965\n",
      "42   2.00     3  5.011760\n",
      "43   2.00     4  3.490937\n",
      "44   2.00     5  4.497301\n",
      "45   5.00     1  5.371057\n",
      "46   5.00     2  5.400965\n",
      "47   5.00     3  5.011760\n",
      "48   5.00     4  3.490937\n",
      "49   5.00     5  4.497301\n",
      "50  10.00     1  5.371057\n",
      "51  10.00     2  5.400965\n",
      "52  10.00     3  5.011760\n",
      "53  10.00     4  3.490937\n",
      "54  10.00     5  4.497301\n",
      "55  20.00     1  5.371057\n",
      "56  20.00     2  5.400965\n",
      "57  20.00     3  5.011760\n",
      "58  20.00     4  3.490937\n",
      "59  20.00     5  4.497301\n",
      "\n",
      "‚úÖ Best alpha based on average MSE: 0.05\n",
      "\n",
      "üìä Final Model Performance:\n",
      "Training MSE: 0.8815\n",
      "Test MSE:     1.2423\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Convert DataFrames to NumPy arrays\n",
    "X_array = X_train.values\n",
    "y_array = y_train.values\n",
    "\n",
    "# Define the list of regularization strengths to try\n",
    "alpha_values = [0.01,0.05,0.1, 0.2, 0.5, 0.75, 1, 1.5, 2, 5, 10, 20]\n",
    "\n",
    "# Set up 5-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Store all MSE results and average MSE per alpha\n",
    "cv_results = []\n",
    "average_mse_per_alpha = {}\n",
    "\n",
    "# Loop through each alpha value\n",
    "for alpha in alpha_values:\n",
    "    mse_per_fold = []\n",
    "\n",
    "    # Perform cross-validation\n",
    "    for fold_index, (train_idx, val_idx) in enumerate(kf.split(X_array), start=1):\n",
    "        # Split the data\n",
    "        X_train_fold, X_val_fold = X_array[train_idx], X_array[val_idx]\n",
    "        y_train_fold, y_val_fold = y_array[train_idx], y_array[val_idx]\n",
    "\n",
    "        # Train Lasso Regression model\n",
    "        model = Lasso(alpha=alpha, random_state=42)\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "        # Predict and calculate MSE\n",
    "        y_pred_fold = model.predict(X_val_fold)\n",
    "        mse = mean_squared_error(y_val_fold, y_pred_fold)\n",
    "\n",
    "        # Save results\n",
    "        mse_per_fold.append(mse)\n",
    "        cv_results.append({\n",
    "            'alpha': alpha,\n",
    "            'fold': fold_index,\n",
    "            'mse': mse\n",
    "        })\n",
    "\n",
    "    # Compute average MSE for current alpha\n",
    "    average_mse_per_alpha[alpha] = np.mean(mse_per_fold)\n",
    "\n",
    "# Print MSE results for all folds\n",
    "print(\"Cross-validation results for each alpha and fold:\")\n",
    "print(pd.DataFrame(cv_results))\n",
    "\n",
    "# Determine the best alpha (smallest average MSE)\n",
    "best_alpha = min(average_mse_per_alpha, key=average_mse_per_alpha.get)\n",
    "print(f\"\\n‚úÖ Best alpha based on average MSE: {best_alpha}\")\n",
    "\n",
    "# Train final model on full training data with best alpha\n",
    "final_model = Lasso(alpha=best_alpha, random_state=42)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on training and test sets\n",
    "y_train_pred = final_model.predict(X_train)\n",
    "y_test_pred = final_model.predict(X_test)\n",
    "\n",
    "# Show training and test MSE\n",
    "print(f\"\\nüìä Final Model Performance:\")\n",
    "print(f\"Training MSE: {mean_squared_error(y_train, y_train_pred):.4f}\")\n",
    "print(f\"Test MSE:     {mean_squared_error(y_test, y_test_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model            | Best Alpha | Training MSE | Test MSE  |\n",
    "|------------------|------------|--------------|-----------|\n",
    "| Linear Regression| ‚Äì          | 0.8563       | 1.3064    |\n",
    "| Ridge Regression | 1          | 0.8579       | 1.2753    |\n",
    "| Lasso Regression | 0.05       | 0.8815       | 1.2423    |\n",
    "\n",
    "We could show, that Lasso and Ridge regression help to reduce overfitting and increase the quality of the model. Lasse Alpha= 0.05 is the best model as it jhas the lowest MSE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
